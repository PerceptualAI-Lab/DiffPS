
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>DiffPS: Diffusion for Person Search</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 1200px;
      margin: 0 auto;
      padding: 20px;
      line-height: 1.6;
      background-color: #f5f5f5;
      color: #333;
    }
    h1, h2, h3 {
      color: #222;
      text-align: center;
    }
    a {
      color: #1a73e8;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .author-block {
      text-align: center;
      margin-bottom: 1em;
    }
    .code-links {
      text-align: center;
      margin: 1.5em 0;
    }
    .code-links a {
      display: inline-block;
      padding: 10px 20px;
      margin: 0 10px;
      background-color: #333;
      color: white;
      text-decoration: none;
      border-radius: 5px;
    }
    .figure {
      text-align: center;
      margin: 2em 0;
    }
    .figure img {
      max-width: 1000px;
      width: 100%;
      display: block;
      margin: 0 auto;
    }
    .bibtex {
      background: #eee;
      padding: 1em;
      font-family: monospace;
      white-space: pre-wrap;
      max-width: 1000px;
      margin: 0 auto;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      max-width: 1000px;
      margin: 0 auto;
    }
    table, th, td {
      border: 1px solid #ccc;
      padding: 8px;
      text-align: center;
    }
    .section {
      margin: 60px auto;
      max-width: 1000px;
      padding: 0 20px;
    }
    .section p {
      text-align: justify;
    }
    .motivation-box {
      background-color: #fff3cd;
      border-left: 6px solid #ffa502;
      padding: 1em 1.5em;
      margin-top: 1em;
      margin-bottom: 2em;
      font-size: 1.05em;
      line-height: 1.7;
      box-shadow: 0 0 5px rgba(0,0,0,0.05);
    }
  </style>
</head>
<body>
  <h1 style="font-size: 2.8em; font-weight: 700;">
    <span style="color: #9932cc;">DiffPS</span><span style="color: #222;"></span>:
    <span style="color: #222;">Leveraging Prior Knowledge of Diffusion Model for Person Search</span>
  </h1>

  <div class="author-block">
    <p>
      <strong>Giyeol Kim<sup>1*</sup></strong>,
      <strong>Sooyoung Yang<sup>2*</sup></strong>,
      <strong>Jihyong Oh<sup>1</sup></strong>,
      <strong>Myungjoo Kang<sup>2,3</sup></strong>,
      <strong>Chanho Eom<sup>1‚Ä†</sup></strong>
      <br>
      <sup>1</sup>GSAIM, Chung-Ang University,
      <sup>2</sup>IPAI, Seoul National University,
      <br>
      <sup>3</sup>Department of Mathematical Sciences and RIMS, Seoul National University
      <br><br>
      *Equal contribution
    </p>
    <p style="text-align: center; font-size: 1.5em; font-weight: bold; color: #d2691e;">üèÜ ICCV 2025 Highlight Paper üéâ</p>
  </div>
  <div class="code-links">
    <a href="https://github.com/PerceptualAI-Lab/DiffPS">üíª Code</a>
    <a href="#">üìÑ Paper</a>
  </div>

  <div class="figure">
    <img src="images/teaser_new.png" alt="Teaser Figure">
    <p><em></em></p>
  </div>

  <div class="section motivation">
    <div class="motivation-box">
      <h2 <em>üöÄ Research Motivation</em></h2>
      <p>
        <strong>Most existing person search models rely on ImageNet pre-trained backbones</strong>. While these backbones provide decent fine-grained features, they often lack the <em>rich visual priors</em> required for person search in diverse and complex scenes.
      </p>
      <p>
        <strong>Furthermore, conventional approaches rely on a shared backbone feature for both person detection and person re-identification</strong> tasks, leading to <em>conflicting optimization objectives</em> and degraded performance.
      </p>
      <p>
        <strong>Our key motivation is to address these limitations by leveraging a pre-trained diffusion model</strong>, which offers richer visual semantics and enables task-specific decoupling to avoid feature interference.
      </p>
      <!-- <ul>
        <li><strong>‚ùå Limitation:</strong> ImageNet backbones lack rich visual priors</li>
        <li><strong>‚ö†Ô∏è Conflict:</strong> Shared backbone causes detection vs re-ID interference</li>
        <li><strong>‚úÖ Our Solution:</strong> Pre-trained <em>diffusion model</em> with task-specific decoupling</li>
      </ul> -->
    </div>
  </div>


  <div class="section">
    <h2>Method</h2>
    <h3 style="text-align: left; margin-top: 1.2em; margin-bottom: 0.3em; color: #444; border-bottom: 1px solid #ccc; padding-bottom: 0.2em;"><em>Prior Knowledge in Diffusion Model</em></h3>
    <p>
<b>DiffPS uses a frozen diffusion model backbone (Stable Diffusion)</b> to provide rich spatial features, and separates task-specific features for detection and Re-ID to avoid gradient interference. This decoupled design ensures stability and better representation learning.
To fully exploit the capabilities of the pre-trained diffusion model, <b>DiffPS leverages four inherent priors embedded in the model architecture</b>:
</p>
<ul style="list-style-type: none; padding-left: 0px;">
  <li>
    <div onclick="toggleContent('text-conditioning')" style="cursor: pointer; font-weight: bold; color: #1a73e8; padding: 0.2em 0;">
      1) Text Conditioning <span>üîΩ</span>
    </div>
    <div id="text-conditioning" style="display: none; margin-left: 1em; margin-top: 0.5em;">
      Pre-trained diffusion models exhibit strong cross-modal alignment between text and image features, enabling precise localization of person-related regions (e.g., body parts, clothing). This alignment enhances the model‚Äôs ability to suppress background clutter and overcome occlusion, providing more robust feature representations for person search.
      <span style="text-decoration: underline;">This motivates us to design <b style="color: #d9534f;">DGRPN</b> and <b style="color: #d9534f;">SFAN</b>,</span> which explicitly exploit this alignment for detection and identification tasks, respectively.
      <img src="images/Text_condition_img.png" alt="Text Conditioning Illustration" style="max-width: 800px; width: 100%; margin-top: 1em;">
    </div>
  </li>
  <li>
    <div onclick="toggleContent('timestep-semantics')" style="cursor: pointer; font-weight: bold; color: #1a73e8; padding: 0.2em 0;">
      2) Timestep Semantics <span>üîΩ</span>
    </div>
    <div id="timestep-semantics" style="display: none; margin-left: 1em; margin-top: 0.5em;">
      The informativeness of diffusion features varies with timestep. For complex scenes like person search, earlier to mid-stage steps (e.g., t=300) offer the best balance between denoising and detail preservation. These steps retain fine-grained features while minimizing the impact of both synthetic and real-world noise, leading to improved person-specific representation.
      <img src="images/Timestep_img.png" alt="Timestep Image Illustration" style="max-width: 800px; width: 100%; margin-top: 1em;">
    </div>
  </li>
  <li>
    <div onclick="toggleContent('hierarchical-structure')" style="cursor: pointer; font-weight: bold; color: #1a73e8; padding: 0.2em 0;">
      3) Hierarchical Structure <span>üîΩ</span>
    </div>
    <div id="hierarchical-structure" style="display: none; margin-left: 1em; margin-top: 0.5em;">
      Diffusion models follow a UNet-based hierarchical structure, where up-stage layers merge global context and local details through skip connections. While up-stage features are generally effective for both detection and re-ID, their usefulness varies across layers. Carefully selecting specific up-stage outputs leads to stronger spatial precision and identity discrimination.
      <img src="images/Hierarchical_img.png" alt="Hierarchical Image Illustration" style="max-width: 800px; width: 100%; margin-top: 1em;">
    </div>
  </li>
  <li>
    <div onclick="toggleContent('shape-bias')" style="cursor: pointer; font-weight: bold; color: #1a73e8; padding: 0.2em 0;">
      4) Shape Bias <span>üîΩ</span>
    </div>
    <div id="shape-bias" style="display: none; margin-left: 1em; margin-top: 0.5em;">
      As noted in <em>‚ÄúIntriguing properties of generative classifiers‚Äù</em>, generative models often exhibit a <strong>shape bias</strong>, emphasizing global structures over fine details. 
      For person search, where both global structure and fine-grained details are essential, leveraging diffusion features while further enhancing high-frequency information can lead to even greater discriminative power
      <span style="text-decoration: underline;">This motivates us to design <b style="color: #d9534f;">MSFRN</b>,</span> which explicitly counteracts the shape bias by reinforcing fine-grained representations.
    </div>
  </li>
</ul>
    <h3 style="text-align: left; margin-top: 1.2em; margin-bottom: 0.3em; color: #444; border-bottom: 1px solid #ccc; padding-bottom: 0.2em;"><em>DiffPS Framework</em></h3>
    <p>
      DiffPS features a frozen diffusion backbone with decoupled detection (<b>DGRPN</b>) and re-ID (<b>MSFRN, SFAN</b>) branches, each tailored to leverage distinct diffusion priors for robust person search.
    </p>
  </div>

  <div class="figure">
    <img src="images/DDPS_arch.png" alt="Architecture Diagram">
    <p><em>DiffPS framework</em></p>
  </div>


  <div class="section">
  <h2 style="text-align: center; margin-bottom: 1em;">üîç Branch-wise Module Design</h2>
  <p style="text-align: center; max-width: 800px; margin: 0 auto 2em;">
    DiffPS decouples the person search task into two branches: detection and re-identification (Re-ID), and designs dedicated modules to fully exploit the strengths of the diffusion backbone.
  </p>

  <div style="display: flex; flex-wrap: wrap; justify-content: space-between; gap: 20px;">
    <!-- MSFRN -->
    <div style="flex: 1; min-width: 280px; background: #f5fff0; padding: 20px; border-left: 6px solid #28a745; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
      <h3 style="color: #28a745;">üîç MSFRN (Re-ID)</h3>
      <p>
        Addressing the <em>shape bias</em> of generative backbones, MSFRN strengthens fine-grained details and high-frequency cues essential for robust person re-identification.
      </p>
    </div>

    <!-- SFAN -->
    <div style="flex: 1; min-width: 280px; background: #fff0f5; padding: 20px; border-left: 6px solid #d63384; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
      <h3 style="color: #d63384;">üß† SFAN (Re-ID)</h3>
      <p>
        SFAN leverages <em>cross-modal alignment</em> from the diffusion model to boost identity discrimination, refining re-ID representations with text-aware semantics.
      </p>
    </div>

    <!-- DGRPN -->
    <div style="flex: 1; min-width: 280px; background: #f0f8ff; padding: 20px; border-left: 6px solid #007acc; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
      <h3 style="color: #007acc;">üìç DGRPN (Detection)</h3>
      <p>
        A novel region proposal network tailored to diffusion features, DGRPN harnesses <em>text-conditioned priors</em> to enhance localization accuracy and suppress background clutter in detection.
      </p>
    </div>
  </div>
</div>

  <div class="figure">
    <h2>Quantitative Results</h2>
    <img src="images/SOTA_results.png" alt="SOTA Results">
  </div>


  <div class="section">
    <p style="text-align: center; font-size: 1.2em; margin-bottom: 1em;">
      üéâ Curious to dive deeper? Check out our full paper above for all the exciting details! üìÑ
    </p>
    <h2>BibTeX</h2>
    <div class="bibtex">
@inproceedings{kim2025diffps,
  title={Leveraging Prior Knowledge of Diffusion Model for Person Search},
  author={Kim, Giyeol and Yang, Sooyoung and Oh, Jihyong and Kang, Myungjoo and Eom, Chanho},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2025}
}
    </div>
  </div>

</body>
<script>
function toggleContent(id) {
  const el = document.getElementById(id);
  const header = el.previousElementSibling;
  const arrow = header.querySelector("span");

  if (el.style.display === 'none' || el.style.display === '') {
    el.style.display = 'block';
    arrow.textContent = 'üîº';
  } else {
    el.style.display = 'none';
    arrow.textContent = 'üîΩ';
  }
}
</script>
</html>
