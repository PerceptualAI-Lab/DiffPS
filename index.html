
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>DiffPS: Diffusion for Person Search</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 1200px;
      margin: 0 auto;
      padding: 20px;
      line-height: 1.6;
      background-color: #f5f5f5;
      color: #333;
    }
    h1, h2, h3 {
      color: #222;
      text-align: center;
    }
    a {
      color: #1a73e8;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .author-block {
      text-align: center;
      margin-bottom: 1em;
    }
    .code-links {
      text-align: center;
      margin: 1.5em 0;
    }
    .code-links a {
      display: inline-block;
      padding: 10px 20px;
      margin: 0 10px;
      background-color: #333;
      color: white;
      text-decoration: none;
      border-radius: 5px;
    }
    .figure {
      text-align: center;
      margin: 2em 0;
    }
    .figure img {
      max-width: 1000px;
      width: 100%;
      display: block;
      margin: 0 auto;
    }
    .bibtex {
      background: #eee;
      padding: 1em;
      font-family: monospace;
      white-space: pre-wrap;
      max-width: 1000px;
      margin: 0 auto;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      max-width: 1000px;
      margin: 0 auto;
    }
    table, th, td {
      border: 1px solid #ccc;
      padding: 8px;
      text-align: center;
    }
    .section {
      margin: 60px auto;
      max-width: 1000px;
      padding: 0 20px;
    }
    .section p {
      text-align: justify;
    }
    .motivation-box {
      background-color: #fff3cd;
      border-left: 6px solid #ffa502;
      padding: 1em 1.5em;
      margin-top: 1em;
      margin-bottom: 2em;
      font-size: 1.05em;
      line-height: 1.7;
      box-shadow: 0 0 5px rgba(0,0,0,0.05);
    }
  </style>
</head>
<body>
  <h1 style="font-size: 2.8em; font-weight: 700;">
    <span style="color: #9932cc;">DiffPS</span><span style="color: #222;"></span>:
    <span style="color: #222;">Leveraging Prior Knowledge of Diffusion Model for Person Search</span>
  </h1>

  <div class="author-block">
    <p>
      <strong>Giyeol Kim<sup>1*</sup></strong>,
      <strong>Sooyoung Yang<sup>2*</sup></strong>,
      <strong>Jihyong Oh<sup>1</sup></strong>,
      <strong>Myungjoo Kang<sup>2,3</sup></strong>,
      <strong>Chanho Eom<sup>1‚Ä†</sup></strong>
      <br>
      <sup>1</sup>GSAIM, Chung-Ang University,
      <sup>2</sup>IPAI, Seoul National University,
      <br>
      <sup>3</sup>Department of Mathematical Sciences and RIMS, Seoul National University
      <br><br>
      *Equal contribution
    </p>
    <p style="text-align: center; font-size: 1.5em; font-weight: bold; color: #d2691e;">üèÜ ICCV 2025 Highlight Paper üéâ</p>
  </div>
  <div class="code-links">
    <a href="https://github.com/PerceptualAI-Lab/DiffPS">üíª Code</a>
    <a href="#">üìÑ Paper</a>
  </div>

  <div class="figure">
    <img src="images/teaser_new.png" alt="Teaser Figure">
    <p><em></em></p>
  </div>

  <div class="section motivation">
    <div class="motivation-box">
      <h2 style="color: #b3568f; margin-top: 0;"><em>üöÄ Research Motivation</em></h2>
      <p>
        Most existing person search models rely on <strong>ImageNet pre-trained backbones</strong>. While these backbones provide decent fine-grained features, they often lack the <em>rich visual priors</em> required for person search in diverse and complex scenes.
      </p>
      <p>
        Furthermore, conventional approaches share the same feature space for both <strong>detection</strong> and <strong>re-identification</strong> tasks, leading to <em>conflicting optimization objectives</em> and degraded performance.
      </p>
      <p>
        <strong>Our key motivation</strong> is to address these limitations by leveraging a <strong>pre-trained diffusion model</strong>, which offers richer visual semantics and enables task-specific decoupling to avoid feature interference.
      </p>
    </div>
  </div>

  <div class="section">
    <h2>Abstract</h2>
    <p>Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images.
      Existing methods predominantly utilize <b>ImageNet pre-trained backbones</b>, which may be less effective at capturing the contextual and fine-grained features crucial for person search.</p>
      <p>Moreover, they rely on a <b>shared backbone feature</b> for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives.
      Recently, diffusion models have emerged as powerful vision backbones, capturing rich visual priors from largescale datasets. </p>
    <p>We propose <b>DiffPS</b>(Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks.</p>
  </div>

  <div class="section">
    <h2>Method</h2>
    <p>DiffPS uses a frozen diffusion model backbone to provide rich spatial features, and separates task-specific features for detection and Re-ID to avoid gradient interference. This decoupled design ensures stability and better representation learning.
    DiffPS tries to get the most out of the pretrained diffusion model's inherent knowledge. The main knowledge DiffPS concentrate on are 1) text-image Alignment and 2) Time step feature characteristics. </p>
  </div>

  <div class="figure">
    <img src="images/DDPS_arch.png" alt="Architecture Diagram">
    <p><em>DiffPS Architecture</em></p>
  </div>

  <div class="figure">
    <h2>Results</h2>
    <img src="images/SOTA_results.png" alt="SOTA Results">
  </div>

  <div class="section">
    <h2>BibTeX</h2>
    <div class="bibtex">
@inproceedings{kim2025diffps,
  title={Leveraging Prior Knowledge of Diffusion Model for Person Search},
  author={Kim, Giyeol and Yang, Sooyoung and Oh, Jihyong and Kang, Myungjoo and Eom, Chanho},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2025}
}
    </div>
  </div>

  <div class="section">
    <h2>Contact</h2>
    <p style="text-align: center;">Email: perceptualai.lab@gmail.com</p>
  </div>
</body>
</html>
